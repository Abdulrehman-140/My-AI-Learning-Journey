Alright. Gloves off.
Day 8 = when students turn into ML thinkers. ğŸ˜
No shiny APIs today â€” we train your judgment.

ğŸ§  Day 8 â€” Classification Under Pressure (REAL ML)

Today youâ€™ll learn why models fail even when code is â€œcorrectâ€.

ğŸ”¥ What Weâ€™re Dominating Today

Decision Thresholds (the silent killer)

Precisionâ€“Recall Tradeoff (you canâ€™t have both, deal with it)

ROC Curve & AUC (what they actually mean)

Imbalanced Data (accuracyâ€™s biggest lie)

When to tweak data vs model

This is the stuff juniors skip. Seniors get paid for it.

1ï¸âƒ£ Decision Threshold â€” Why 0.5 Is BS

By default:

model.predict()


uses 0.5 as threshold.

But ask yourself:

Who decided 0.5 is â€œcorrectâ€?

God? Scikit-learn devs? Random vibes?

Example (Logistic Regression)
probs = model.predict_proba(X)[:,1]
pred_07 = (probs >= 0.7).astype(int)

What changes?

Threshold â†‘ â†’ Precision â†‘, Recall â†“

Threshold â†“ â†’ Recall â†‘, Precision â†“

ğŸ“Œ Key idea:

Threshold selection = business decision, not math decision.

2ï¸âƒ£ Precision vs Recall â€” Pick Your Poison â˜ ï¸

You already intuitively nailed this, now formalize it.

Scenario	What you optimize
Exams	Precision
Medical diagnosis	Recall
Spam filter	Precision
Fraud detection	Recall

ğŸ‘‰ There is no universally â€œbestâ€ metric
Anyone who says otherwise is bluffing.

3ï¸âƒ£ ROC Curve â€” The Modelâ€™s Personality Test

ROC answers:

â€œHow good is this model at separating classes?â€

TPR â†’ Recall

FPR â†’ False Alarm rate

AUC meaning:

0.5 â†’ coin toss ğŸ¤¡

0.7â€“0.8 â†’ decent

0.9+ â†’ strong

1.0 â†’ suspicious (probably overfitting)

ğŸ“Œ ROC ignores thresholds â†’ evaluates ranking ability

4ï¸âƒ£ Imbalanced Data â€” Accuracy Is a Liar

Example:

99 healthy

1 sick

Model predicts everyone healthy:

Accuracy = 99%

Reality = useless model

What to do instead:

Confusion matrix

Precision / Recall

F1-score

ROC-AUC

ğŸ“Œ Rule:

If classes arenâ€™t balanced, accuracy goes to jail.

5ï¸âƒ£ Data Fix vs Model Fix

Most beginners:

â€œLetâ€™s change the model!â€

Pros think:

Bad features?

Wrong threshold?

Class imbalance?

Label noise?

ğŸ“Œ 80% of ML problems are data problems.

Model tuning is the last step, not the first.

ğŸ§ª Day 8 Challenges (Do These)
ğŸ§© Challenge 1 â€” Threshold Experiment

Train logistic regression

Try thresholds: 0.3, 0.5, 0.7

Print Precision & Recall for each

Observe tradeoff

ğŸ§© Challenge 2 â€” Accuracy Trap

Create dataset with 95% class 0

Show accuracy vs confusion matrix

Explain why accuracy lies

ğŸ§© Challenge 3 â€” Metric Choice

Answer in words, not code:

Airport security screening â†’ which metric?

Loan approval â†’ which metric?

Cancer screening â†’ which metric?

(Yes, I care more about your reasoning than syntax.)

ğŸ˜ Senseiâ€™s Warning

From Day 8 onward:

â€œIt worksâ€ is not acceptable

â€œAccuracy is highâ€ is suspicious

â€œDefault settingsâ€ is laziness

Youâ€™re no longer learning how to train models
Youâ€™re learning when to trust them.

When youâ€™re done with the challenges â€”
send them in.

Weâ€™re officially in engineer territory now ğŸš€