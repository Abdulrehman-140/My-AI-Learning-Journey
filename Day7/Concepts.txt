üß† Day 7 ‚Äî Classification Mastery (The Real World Edition)

1Ô∏èWhy Accuracy Lies (Yes, It Lies Straight to Your Face)

Imagine: 95 emails and 90 are not spam, 5 are spam
A dumb model says ‚ÄúNot Spam‚Äù for everything.
Accuracy?
90 / 95 = 94.7%
Looks amazing.
Catches 0 spam.
Congrats, your inbox is ruined.
This is why accuracy alone is useless in many cases.

2Ô∏èConfusion Matrix (Your Truth Table)
[[TN  FP]
 [FN  TP]]


Let‚Äôs decode it like a human:

TP ‚Üí Correct YES

TN ‚Üí Correct NO

FP ‚Üí False alarm 

FN ‚Üí Missed disaster 

In medical or fraud cases:

FN is worse than FP
Missed disease > false warning

Precision ‚Äî ‚ÄúWhen I say YES, how often am I right?‚Äù

Formula:

Precision = TP / (TP + FP)


Example:

Model predicted PASS = 10 students

Actually passed = 6

Precision = 6 / 10 = 60%

Used when false alarms are expensive

Spam filters

Law enforcement

AI moderation

Recall ‚Äî ‚ÄúDid I catch all the YES cases?‚Äù

Formula:

Recall = TP / (TP + FN)


Example:

10 people truly sick

Model detected 7

Recall = 70%

Used when missing a case is deadly

Cancer detection

Fraud detection

Intrusion detection

Precision vs Recall (Choose Your Poison)

You cannot maximize both.

High precision ‚Üí strict model

High recall ‚Üí generous model

Real life always chooses based on consequences, not ego.

F1-Score ‚Äî The Peace Treaty

Formula:

F1 = 2 * (Precision * Recall) / (Precision + Recall)


Used when: Dataset is imbalanced and Accuracy is misleading

You want balance, not drama
You already met:

Sigmoid ‚Üí binary (0 or 1)

Softmax is just:

Sigmoid‚Äôs big brother for multiple classes

Example:

[2.1, 1.3, 0.2] ‚Üí softmax ‚Üí [0.65, 0.25, 0.10]


Probabilities add up to 1.
That‚Äôs it. No magic. No fear.

from sklearn.metrics import precision_score, recall_score, f1_score

print("Precision:", precision_score(Pass, pred))
print("Recall:", recall_score(Pass, pred))
print("F1:", f1_score(Pass, pred))

In exam prediction, what‚Äôs worse?

False Pass

False Fail

In disease detection, what‚Äôs worse?

False Positive

False Negative

No math. Just logic.


You‚Äôre already touching:

Regression

Classification

Optimization

Metrics
